{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEwTzkXqUMXw"
      },
      "source": [
        "import json\n",
        "import collections\n",
        "import re\n",
        "import os \n",
        "import wget"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9Zcmo5wGzQi",
        "outputId": "1bdbdb31-9161-4515-e392-b736ec8ad8df"
      },
      "source": [
        "! pip install wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=cb6fab97e1cfb4ac95579836ae870d97aea9824a0725c949dd14e8332b0e7ebb\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwx8v8U0GeM5"
      },
      "source": [
        "url_data = 'https://raw.githubusercontent.com/caterinaLacerra/word2vec_data/master/wiki_10k.txt'\n",
        "train_data_path=wget.download(url_data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jiEzUb2G8tC"
      },
      "source": [
        "class Word2VecDataset():\n",
        "    def __init__(self,txt_path,vocab_size,unk_token,window_size):\n",
        "        self.window_size=window_size \n",
        "        self.data_words=self.read_data(txt_path)\n",
        "        self.build_vocabulary(vocab_size,unk_token)\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        txt_path(str): Path to the raw-txt file.\n",
        "        vocab_size(int):Maximum amount of words that we want to embed.\n",
        "        ink_token(str): how will unknown words represented (e.g. 'UNK').\n",
        "        window_size(int): Number of words to consider as context.\n",
        "        \"\"\"\n",
        "   \n",
        "    def __iter__(self):\n",
        "        words=self.data_words\n",
        "        # each li is a list in words\n",
        "        for li in words:\n",
        "          len_li=len(li)\n",
        "\n",
        "          for index in range(len_li):\n",
        "            c_word=li[index]\n",
        "            # this word sould be in a vovabulary\n",
        "            if c_word in self.word2id:\n",
        "              min_index=max(0,index -self.window_size)\n",
        "              max_index=min(len_li,index +self.window-size)\n",
        "          window_index=[j for j in range(min_index,max_index) if j !=index]\n",
        "          for k in window_index:\n",
        "            if li[k] in self.word2id:\n",
        "              # index of k word in vocab\n",
        "              predict_id=self.word2id[li[k]]\n",
        "              # index of input word in vocab\n",
        "              c_word_id=self.word2id[c_word]\n",
        "              dic_ok={'predict': predict-id, 'inputs':c_word_id}\n",
        "\n",
        "              yield dic_ok\n",
        "\n",
        "\n",
        "    \n",
        "    def read_data(self,txt_path):\n",
        "      \"\"\"converts each line in the input file into list of lists of tokenized words.\"\"\"\n",
        "      with open(txt_path) as f:\n",
        "            data=[]\n",
        "            total_words=0\n",
        "            for i in f:\n",
        "                split=self.tokenize(i) \n",
        "                if split:\n",
        "                    data.append(split)\n",
        "                    total_words+=len(split)\n",
        "      return data\n",
        "\n",
        "    # the pen is on the table--->['the','pen','is','on','the','table']\n",
        "    def tokenize(self,i,pattern='\\w+'):\n",
        "      \"\"\"tokenize a single line\"\"\"  \n",
        "      return [word.lower() for word in re.compile(pattern).findall(i.lower()) if word]\n",
        "      \n",
        "        \n",
        "    def build_vocabulary(self,vocab_size,unk_token):\n",
        "      \"\"\" defines the vocabulary to be used. builds a mapping (words,index) for\n",
        "      each word in the vocabulary.\n",
        "      Args:\n",
        "      vocab_size(int): size of the vocabulary\n",
        "      unk_token(str): token to aassociate with unktoken words\n",
        "      \"\"\"\n",
        "      counter_list=[]\n",
        "      #contex is a list of tokens within a single sentence\n",
        "      for context in self.data_words:\n",
        "        counter_list.extend(context)\n",
        "      counter=collections.Counter(counter_list)\n",
        "      counter_len=len(counter)\n",
        "      print('Number of distinct words: {}'.format(counter_len))\n",
        "\n",
        "      # consider only the (vocab size-1) most commen words to build the vocab\n",
        "      # dictionary with (word,index) \n",
        "      dictionary={k: i for i,(k, _) in enumerate(counter.most_common(vocab_size-1))}\n",
        "      assert unk_token not in dictionary\n",
        "      # all other words are mapped to UNK\n",
        "      dictionary[unk_token]=vocab_size-1\n",
        "      self.word2id=dictionary\n",
        "\n",
        "      #dictionary with (word,frequency) pairs--including only words that are in the dictionary\n",
        "      dict_counts={k: counter[k] for k in dictionary if k is not unk_token}\n",
        "      self.frequency=dict_counts\n",
        "      self.tot_occurrences=sum(dict_counts[k] for k in dict_counts)\n",
        "\n",
        "      print('Total occurrences of words in dictionary:{}'.format(self.tot_occurrences))\n",
        "      less_freq_word=min(dict_counts,key=counter.get)\n",
        "      print('Less frequent word in dictionary appears {} times ({})'.format(dict_counts[less_freq_word],less_freq_word))\n",
        "\n",
        "      #index to word\n",
        "      self.id2word={v: k for k,v in dictionary.items()}\n",
        "\n",
        "      #data is the text converted to indexes, as list of list\n",
        "\n",
        "      data=[]\n",
        "      # for each sentence \n",
        "      for i,v in enumerate(self.data_words):\n",
        "        l=[]\n",
        "        # for each word in the sentence\n",
        "        for j,x in enumerate(self.data_words[i]):\n",
        "          l.append(j)\n",
        "        data.append(l)\n",
        "      self.data_idx=data\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "       \n",
        "        \n",
        "        "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oesSxuMB6tvj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmVYRNVKHObf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0ee9bb-be7d-44dc-b459-dab33d575423"
      },
      "source": [
        "dataset=Word2VecDataset(train_data_path,10000,'UNK',5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of distinct words: 30501\n",
            "Total occurrences of words in dictionary:228841\n",
            "Less frequent word in dictionary appears 2 times (tasked)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX2pX2w0PsZm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}